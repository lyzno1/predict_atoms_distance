{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minor-joint",
   "metadata": {},
   "source": [
    "# High-dimensional Bayesian Optimization of CrabNet Hyperparameters\n",
    "###### Created March 5, 2022\n",
    "\n",
    "![logo](matbench_logo.png)\n",
    "\n",
    "\n",
    "# Description\n",
    "\n",
    "Use of hyperparameter Bayesian optimization with [CrabNet](https://crabnet.readthedocs.io/) revealed favorable results in\n",
    "improving the error over the baseline model; however, only 100 iterations were used to\n",
    "optimize 23 hyperparameters with little prior information to make an informed decision\n",
    "(for example, the default CrabNet hyperparameters were not available to the\n",
    "hyperparameter optimization algorithm). Only the constraints on the hyperparameter\n",
    "search space, which were quite generous, would constitute prior information, and 100\n",
    "iterations across 23 parameters is a very sparse sampling. Here, we describe a more direct comparison to\n",
    "the relatively new high-dimensional Bayesian optimization algorithm called Sparse\n",
    "Axis-Aligned Subspace Bayesian Optimization, or SAASBO. We use 10 Sobol trials and 90 Bayesian trials with the default Gaussian Process Expected Improvement (GPEI) model to be more comparable to the results with SAASBO which also used 10 Sobol trials and 90 Bayesian trials.\n",
    "\n",
    "[facebook/Ax](https://github.com/facebook/Ax) is used as the backend for performing\n",
    "SAASBO. For additional files related to this `matbench` submission, see the\n",
    "[crabnet-hyperparameter](https://github.com/sparks-baird/crabnet-hyperparameter)\n",
    "repository. Due to the procedure being relatively expensive (esp. due to use of\n",
    "nested-CV when training CrabNet) and requiring a few days of runtime on two RTX 2080 Ti\n",
    "GPUs, this notebook demonstrates how to submit jobs to a high-performance computing\n",
    "center (in our case, CHPC @ University of Utah). `submitit` parameters such as `account`\n",
    "and `partition` might vary somewhat for your own HPC center.\n",
    "\n",
    "# Benchmark name\n",
    "Matbench v0.1\n",
    "\n",
    "# Package versions\n",
    "- matplotlib==3.5.0\n",
    "- pandas==1.3.5\n",
    "- ax-platform==0.2.3\n",
    "- pyro-ppl==1.8.0\n",
    "- plotly==5.5.0\n",
    "- crabnet==1.2.5\n",
    "- scikit-learn\n",
    "- submitit==1.4.1\n",
    "- matbench==0.5\n",
    "- cloudpickle==2.0.0\n",
    "\n",
    "# Algorithm description\n",
    "Recently, SAASBO has been demonstrated to be a highly effect high-dimensional Bayesian\n",
    "optimization scheme. Here, we compare against [Ax/SAASBO Bayesian adaptive design](https://ax.dev/tutorials/saasbo.html) to simultaneously optimize 23\n",
    "hyperparameters of\n",
    "[CrabNet](https://crabnet.readthedocs.io/). `100` sequential design iterations were used, and parameters were chosen based\n",
    "on a combination of intuition and algorithm/data constraints (e.g. elemental featurizers\n",
    "which were missing elements contained in the dataset were removed). The first `10`\n",
    "iterations were based on SOBOL sampling to create a rough initial model, while the\n",
    "remaining `90` iterations were GPEI Bayesian adaptive design iterations. For the inner\n",
    "loops (where hyperparameter optimization is performed), the average MAE across each of\n",
    "the *five inner folds* was used as Ax's objective to minimize. The best parameter set\n",
    "was then trained on all the inner fold data and used to predict on the test set (unknown\n",
    "during hyperparameter optimization). This is nested cross-validation (CV), and is\n",
    "computationally expensive. See [automatminer: running a\n",
    "benchmark](https://hackingmaterials.lbl.gov/automatminer/advanced.html#running-a-benchmark)\n",
    "for more information on nested CV.\n",
    "\n",
    "# Relevant citations\n",
    "- CrabNet: [Wang et al.](https://doi.org/10.1038/s41524-021-00545-1)\n",
    "- SAASBO: [Eriksson and Jankowiak](\n",
    "https://doi.org/10.48550/arXiv.2103.00349)\n",
    "\n",
    "# Related Matbench Submissions\n",
    "- [`matbench_v0.1_Ax_SAASBO_CrabNet_v1.2.7`](https://github.com/materialsproject/matbench/tree/main/benchmarks/matbench_v0.1_Ax_CrabNet_v1.2.1)\n",
    "- [`matbench_v0.1_Ax_CrabNet_v1.2.1`](https://github.com/materialsproject/matbench/tree/main/benchmarks/matbench_v0.1_Ax_CrabNet_v1.2.1)\n",
    "- [`matbench_v0.1_CrabNet_v1.2.1`](https://github.com/materialsproject/matbench/tree/main/benchmarks/matbench_v0.1_CrabNet_v1.2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c0aeb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "certified-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% imports\n",
    "# NOTE: `pip install pyro-ppl` to use FULLYBAYESIAN (SAASBO)\n",
    "from submitit import AutoExecutor\n",
    "import cloudpickle as pickle\n",
    "from utils.matbench import matbench_fold_GPEI, collect_results, task, savepath, dummy\n",
    "print(f\"dummy: {dummy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500bb193",
   "metadata": {},
   "source": [
    "# Job Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b9805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% submission\n",
    "log_folder = \"log_ax/%j\"\n",
    "walltime = 28 * 60  # 4320  # 4320 min == 3 days\n",
    "# partition, account = [\"notchpeak-gpu-guest\", \"owner-gpu-guest\"]\n",
    "partition, account = [\"notchpeak-gpu\", \"notchpeak-gpu\"]\n",
    "# partition, account = [\"notchpeak-guest\", \"owner-guest\"]\n",
    "executor = AutoExecutor(folder=log_folder)\n",
    "executor.update_parameters(\n",
    "    timeout_min=walltime,\n",
    "    slurm_partition=partition,\n",
    "    slurm_gpus_per_task=1,\n",
    "    slurm_mem_per_gpu=6000,\n",
    "    slurm_cpus_per_gpu=4,\n",
    "    slurm_additional_parameters={\"account\": account},\n",
    ")\n",
    "jobs = executor.map_array(matbench_fold_GPEI, task.folds)  # sbatch array\n",
    "job_ids = [job.job_id for job in jobs]\n",
    "# https://www.hpc2n.umu.se/documentation/batchsystem/job-dependencies\n",
    "job_ids_str = \":\".join(job_ids)  # e.g. \"3937257_0:3937257_1:...\"\n",
    "\n",
    "with open(\"jobs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(jobs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a417845",
   "metadata": {},
   "source": [
    "# Collect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d71566",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_folder = \"log_matbench/%j\"\n",
    "walltime = 10\n",
    "collector = AutoExecutor(folder=collect_folder)\n",
    "collector.update_parameters(\n",
    "    timeout_min=walltime,\n",
    "    slurm_partition=partition,\n",
    "    slurm_additional_parameters={\n",
    "        \"account\": account,\n",
    "        \"dependency\": f\"afterok:{job_ids_str}\",\n",
    "    },\n",
    ")\n",
    "collector_job = collector.submit(collect_results, False)  # sbatch array\n",
    "\n",
    "print(\n",
    "    f\"Waiting for submission jobs ({job_ids_str}) to complete before running collector job ({collector_job.job_id}). Use the matbench output file that will be saved to {savepath} after all jobs have run.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
